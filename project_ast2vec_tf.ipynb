{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_ast2vec_tf.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4AHzbcP1lO9P"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhKFf0lVSqex",
        "outputId": "bb5eb39e-7876-4c57-8032-c380b9bc4f04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr 27 05:29:40 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if861thUcH7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0481fdeb-b964-46ae-da22-e8fa62804082"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28LF6IB7Rza4",
        "outputId": "07d00d57-d9a0-48b9-b5cd-3bdf8471e2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiUMpooHj5YG"
      },
      "source": [
        "!cd /content/drive/MyDrive/ast2vec_tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd /content/drive/MyDrive/major-proj/ast2vec_tf"
      ],
      "metadata": {
        "id": "m9tC0ac3k0-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ed2589-ecec-43a3-82d9-873fa4e36eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/major-proj/ast2vec_tf'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiiIXxKRmYGX"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/ast2vec_tf')\n",
        "# import tensorflow as tf\n",
        "# import tensorflow.keras.layers as layers\n",
        "# from collections import OrderedDict\n",
        "\n",
        "# import python_ast_utils\n",
        "# import tree as tree\n",
        "# import tree_grammar\n",
        "# import numpy as np\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KERAS"
      ],
      "metadata": {
        "id": "4AHzbcP1lO9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class RTGAELSTM(tf.keras.Model):\n",
        "  def __init__(self, grammar, dim):\n",
        "        super(RTGAELSTM, self).__init__()\n",
        "        self.grammar = grammar\n",
        "        self.dim = dim\n",
        "        # set up a tree parser for the given grammar. We need this for\n",
        "        # training the autoencoder\n",
        "        self.parser_ = tree_grammar.TreeParser(grammar)\n",
        "\n",
        "\n",
        "        # set up a map from symbols to indices\n",
        "        self.symbol_to_idx_ = {}\n",
        "        symbols = list(self.grammar._alphabet.keys())\n",
        "        for i in range(len(symbols)):\n",
        "            self.symbol_to_idx_[symbols[i]] = i\n",
        "\n",
        "        # set up a GRU to encode siblings\n",
        "        self.enc_siblings_ = layers.LSTM(self.dim,return_state=True)\n",
        "        # set up a GRU to encode to parents\n",
        "        self.enc_parents_  = layers.LSTM(self.dim,return_state=True)\n",
        "        # set up a GRU to decode siblings\n",
        "        self.dec_siblings_ = layers.LSTM(self.dim,return_state=True)\n",
        "        self.dec_siblings_out_ = layers.Dense(self.dim, input_shape=(self.dim,))\n",
        "        # set up a GRU to decode from parents\n",
        "        # self.dec_parents_ = \n",
        "        self.dec_parents_  = layers.LSTM(self.dim,return_state=True)\n",
        "        # set up linear layers to decide which grammar rule to take\n",
        "        # from a given nonterminal symbol\n",
        "        self.dec_cls_ = OrderedDict()\n",
        "        for nont in self.grammar._nonts:\n",
        "            self.dec_cls_[nont] = layers.Dense(len(self.grammar._rules[nont]), input_shape=(self.dim,), activation = 'softmax')\n",
        "        # also set up classifiers for optional and starred rules\n",
        "        for lhs in self.grammar._rules:\n",
        "            for rhs in self.grammar._rules[lhs]:\n",
        "                for nont in rhs[1]:\n",
        "                    if nont not in self.dec_cls_ and isinstance(nont, str) and (nont.endswith('*') or nont.endswith('?')):\n",
        "                        self.dec_cls_[nont] = layers.Dense(2, input_shape=(self.dim,))\n",
        "\n",
        "  def encode(self, x):\n",
        "        # first encode the sequence of children recursively\n",
        "        if len(x._children) == 0:\n",
        "            h_child = tf.Variable(tf.zeros([1,self.dim]))\n",
        "        else:\n",
        "            H_child = tf.Variable(tf.zeros([len(x._children), self.dim]))\n",
        "            for k in range(len(x._children)):\n",
        "                # aa=aa[2].assign(1)\n",
        "                H_child = H_child[k,:].assign(self.encode(x._children[k])[0])\n",
        "                # H_child[k, :] = self.encode(x._children[k])\n",
        "            _, h_child,_ = self.enc_siblings_(tf.expand_dims(H_child, axis=0))\n",
        "            # h_child = tf.expand_dims(h_child, axis=[0])\n",
        "        # then, encode to a parent representation\n",
        "        indices = [self.symbol_to_idx_[x._label]]\n",
        "        depth = len(self.symbol_to_idx_)\n",
        "        label_encoding = tf.expand_dims(tf.one_hot(indices, depth), axis=1)\n",
        "        _, h_parent,_ = self.enc_parents_(inputs=label_encoding, initial_state=[h_child,h_child])\n",
        "        return h_parent\n",
        "\n",
        "  def decode(self, h, max_size = None):\n",
        "        x, N = self.decode_(h, self.grammar._start, max_size)\n",
        "        return x\n",
        "\n",
        "  def decode_(self, h, nont, max_size = None):\n",
        "        \n",
        "        # first, decide the grammar rule that shall be applied\n",
        "        r = np.argmax(self.dec_cls_[nont](h))\n",
        "        rule = self.grammar._rules[nont][r]\n",
        "        # generate a new tree object with the label determined by the\n",
        "        # grammar rule\n",
        "        label = rule[0]\n",
        "        child_nonts = rule[1]\n",
        "        x = tree.Tree(label)\n",
        "        # map from parent to child state\n",
        "\n",
        "        indices = [self.symbol_to_idx_[x._label]]\n",
        "        depth = len(self.symbol_to_idx_)\n",
        "        label_encoding = tf.expand_dims(tf.one_hot(indices, depth), axis=1)\n",
        "        _, h_child,_ = self.dec_parents_(inputs=label_encoding, initial_state=[h,h])\n",
        "        # generate all children that are determined by the grammar rule\n",
        "        child_nonts = rule[1]\n",
        "        N = 1\n",
        "        if max_size is not None:\n",
        "            max_size -= 1\n",
        "        for k in range(len(child_nonts)):\n",
        "            if isinstance(child_nonts[k], str) and child_nonts[k].endswith('*'):\n",
        "                # if the next child is starred, continue decoding children for\n",
        "                # this nonterminal until the classifier says otherwise\n",
        "                while max_size is None or max_size > 0:\n",
        "                    continue_flag = np.argmax(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                    if continue_flag == 0:\n",
        "                        break\n",
        "                    # infer state for the next child\n",
        "                    hk = self.dec_siblings_out_(h_child)\n",
        "                    # decode it recursively and append it\n",
        "                    y, Nk = self.decode_(hk, child_nonts[k][:-1], max_size)\n",
        "                    x._children.append(y)\n",
        "                    # adjust the remaining maximum size\n",
        "                    if max_size is not None:\n",
        "                        max_size -= Nk\n",
        "                    N += Nk\n",
        "                    # update the state\n",
        "                    hk = tf.expand_dims(hk,axis=0)\n",
        "                    _, h_child,_ = self.dec_siblings_(inputs=hk, initial_state=[h_child,h_child])\n",
        "                # once all children for the starred nonterminal are decoded,\n",
        "                # continue with the next child nonterminal\n",
        "                continue\n",
        "            elif isinstance(child_nonts[k], str) and child_nonts[k].endswith('?'):\n",
        "                # if the next child is optional, check if it shall be decoded\n",
        "                continue_flag = np.argmax(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                if continue_flag == 0:\n",
        "                    # if not, continue right away\n",
        "                    continue\n",
        "                nont = child_nonts[k][:-1]\n",
        "            else:\n",
        "                nont = child_nonts[k]\n",
        "            # if the max size is exceeded, leave the nonterminals\n",
        "            if max_size is not None and max_size <= 0:\n",
        "                x._children.append(tree.Tree(nont))\n",
        "                continue\n",
        "            # at this point, we know that the child shall be decoded regularly.\n",
        "            # First, we infer the state for the next child\n",
        "            hk = self.dec_siblings_out_(h_child)\n",
        "            # then, we decode it recursively and append it\n",
        "            y, Nk = self.decode_(hk, nont, max_size)\n",
        "            x._children.append(y)\n",
        "            # adjust the remaining maximum size\n",
        "            if max_size is not None:\n",
        "                max_size -= Nk\n",
        "            N += Nk\n",
        "            # update the state\n",
        "            hk = tf.expand_dims(hk,axis=0)\n",
        "            _, h_child, _ = self.dec_siblings_(inputs=hk, initial_state=[h_child,h_child])\n",
        "        # return\n",
        "        return x, N\n",
        "\n",
        "  def decode_forced_(self, h, nont, seq, t, score_list, decoding_list = None):\n",
        "        # first, decide the current scores\n",
        "        if decoding_list is not None:\n",
        "            decoding_list.append(h)\n",
        "        score_list.append(self.dec_cls_[nont](h))\n",
        "        # then, retrieve the rule that ought to be applied\n",
        "        r = seq[t]\n",
        "        t += 1\n",
        "        rule = self.grammar._rules[nont][r]\n",
        "        # generate a new tree object with the label determined by the\n",
        "        # grammar rule\n",
        "        label = rule[0]\n",
        "        child_nonts = rule[1]\n",
        "        # map from parent to child state\n",
        "\n",
        "        indices = [self.symbol_to_idx_[label]]\n",
        "        depth = len(self.symbol_to_idx_)\n",
        "        label_encoding = tf.expand_dims(tf.one_hot(indices, depth), axis=1)\n",
        "        # label_encoding = torch.zeros(len(self.symbol_to_idx_))\n",
        "        # label_encoding[self.symbol_to_idx_[label]] = 1.\n",
        "        _, h_child,_ = self.dec_parents_(inputs=label_encoding, initial_state=[h,h])\n",
        "        # generate all children that are determined by the grammar rule\n",
        "        child_nonts = rule[1]\n",
        "        for k in range(len(child_nonts)):\n",
        "            if isinstance(child_nonts[k], str) and child_nonts[k].endswith('*'):\n",
        "                # if the next child is starred, continue decoding children for\n",
        "                # this nonterminal until the rule sequence says otherwise\n",
        "                score_list.append(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                while seq[t] == 1:\n",
        "                    t += 1\n",
        "                    # infer state for the next child\n",
        "                    hk = self.dec_siblings_out_(h_child)\n",
        "                    # decode the current child\n",
        "                    t = self.decode_forced_(hk, child_nonts[k][:-1], seq, t, score_list, decoding_list)\n",
        "                    # update the state\n",
        "                    hk = tf.expand_dims(hk,axis=0)\n",
        "                    _, h_child,_ = self.dec_siblings_(inputs=hk, initial_state=[h_child,h_child])\n",
        "                    score_list.append(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                t += 1\n",
        "                # once all children for the starred nonterminal are decoded,\n",
        "                # continue with the next child nonterminal\n",
        "                continue\n",
        "            elif isinstance(child_nonts[k], str) and child_nonts[k].endswith('?'):\n",
        "                # if the next child is optional, check if it shall be decoded\n",
        "                score_list.append(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                if seq[t] == 0:\n",
        "                    t += 1\n",
        "                    continue\n",
        "                t += 1\n",
        "                nont = child_nonts[k][:-1]\n",
        "            else:\n",
        "                nont = child_nonts[k]\n",
        "            # at this point, we know that the child shall be decoded regularly.\n",
        "            # First, we infer the state for the next child\n",
        "            hk = self.dec_siblings_out_(h_child)\n",
        "            # then, we decode it recursively\n",
        "            t = self.decode_forced_(hk, nont, seq, t, score_list, decoding_list)\n",
        "            # update the state\n",
        "            hk = tf.expand_dims(hk,axis=0)\n",
        "            _, h_child, _ = self.dec_siblings_(inputs=hk, initial_state=[h_child,h_child])\n",
        "        # return\n",
        "        return t\n",
        "\n",
        "  def pad_seq(self, t):\n",
        "    t_pad = []\n",
        "    maxshape = max([j.shape[1] for j in t])\n",
        "    for i in t:\n",
        "      t2 = tf.zeros([1,maxshape-i.shape[1]],dtype=i.dtype)\n",
        "      i_pad = tf.concat([i, t2], 1)\n",
        "      t_pad.append(i_pad)\n",
        "    return tf.squeeze(tf.stack(t_pad, axis=0), axis = 1), maxshape\n",
        "\n",
        "  def one_hot_seq(self,t,depth):\n",
        "    t_one = []\n",
        "    for i in t:\n",
        "        i_one = tf.one_hot([i], depth)\n",
        "        t_one.append(i_one)\n",
        "    return tf.squeeze(tf.stack(t_one, axis=0), axis = 1)\n",
        "\n",
        "  \n",
        "  def compute_loss(self, x):\n",
        "        h = self.encode(x)\n",
        "        nont, seq = self.parser_.parse_tree(x)\n",
        "        scores = []\n",
        "        self.decode_forced_(h, nont, seq, 0, scores)\n",
        "        scores,depth = self.pad_seq(scores)\n",
        "        seq = self.one_hot_seq(seq,depth)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(seq,scores,from_logits=False, label_smoothing=0, axis=-1)\n",
        "        tot_loss = sum(loss.numpy())\n",
        "        return tot_loss\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.compute_loss(inputs)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "P6upIWJ8hvSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmAGNO4Xm8Ow"
      },
      "source": [
        "model = RTGAELSTM(python_ast_utils.grammar, dim = 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEmeUhNyk_W3"
      },
      "source": [
        "def encode_trees(model, trees):\n",
        "      X = np.zeros((len(trees), 256))\n",
        "      # iterate over all trees\n",
        "      for i in range(len(trees)):\n",
        "          # encode the current tree\n",
        "          x = model.encode(trees[i])\n",
        "          # convert the vector to numpy and store it in the matrix\n",
        "      # return the matrix\n",
        "      return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "programs     = ['arr = input().split()\\n']\n",
        "trees, programs_to_trees = python_ast_utils.parse_asts(programs, filter_uniques = True)\n",
        "vec=encode_trees(model, trees)\n",
        "print(trees,\"\\n\\n\\n\\n\",vec)"
      ],
      "metadata": {
        "id": "IEMooUjKR2CK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trees"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtmRlG_b9Z21",
        "outputId": "9bdaa99e-cb33-466a-b5d3-800a189ceff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Module(Assign(Name, Call(Attribute(Call(Name)))))]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_vec():\n",
        "  rng = np.random.default_rng()\n",
        "  vec = tf.convert_to_tensor(rng.random((1, 256)), dtype=tf.float32)\n",
        "  # vec = ( (vec  - 0 ) / (1 - 0) ) * (1 - -1) + -1\n",
        "  return vec"
      ],
      "metadata": {
        "id": "WsAWc4zIhDJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB7qTxsVPCEL"
      },
      "source": [
        "def decode_points(model, X, max_size = 10):\n",
        "    trees = model.decode(X, max_size = max_size)\n",
        "    return trees\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = generate_vec()\n",
        "tre=decode_points(model,vec)\n",
        "tre"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJ0nAlPfgNl5",
        "outputId": "88bfa34c-6fb5-41f7-95bc-0eda44818350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Module(AugAssign(Set, FloorDiv, GeneratorExp(DictComp(Constant, Constant, comprehension(SetComp(expr), expr)))))"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89-haIepuFch"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compute_loss(tre)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "IzsJMa6gd9Dz",
        "outputId": "ebf9f84c-39b7-4fa2-8f1e-17e58d4c7059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-492b2134360b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0mnont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# aa=aa[2].assign(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mH_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# H_child[k, :] = self.encode(x._children[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_child\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_siblings_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# aa=aa[2].assign(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mH_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# H_child[k, :] = self.encode(x._children[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_child\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_siblings_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# aa=aa[2].assign(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mH_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# H_child[k, :] = self.encode(x._children[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_child\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_siblings_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# aa=aa[2].assign(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mH_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# H_child[k, :] = self.encode(x._children[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_child\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_siblings_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# aa=aa[2].assign(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mH_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# H_child[k, :] = self.encode(x._children[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_child\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_siblings_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0;31m# aa=aa[2].assign(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m                 \u001b[0mH_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m                 \u001b[0;31m# H_child[k, :] = self.encode(x._children[k])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_child\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_siblings_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-9b75783967b6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# h_child = tf.expand_dims(h_child, axis=[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# then, encode to a parent representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol_to_idx_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbol_to_idx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mlabel_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'expr'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Es-ujCELvANu"
      },
      "source": [
        "# vec=encode_trees(modellstm, trees)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUjcyGDDvJvG",
        "outputId": "9f2c3b92-70bf-4954-c838-2ad128b8ed29"
      },
      "source": [
        "print(\"hi rahul457\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi rahul457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCGW6SrrIZY9"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWcnszHoLMaB",
        "outputId": "d6a31066-e575-49b3-bbb4-18bff20f3d8c"
      },
      "source": [
        "x=trees[0]\n",
        "h = model.encode(x)\n",
        "nont, seq = model.parser_.parse_tree(x)\n",
        "scores = []\n",
        "print(model.decode_forced_(h, nont, seq, 0, scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfJO8c3tVt0G",
        "outputId": "6a407af1-8450-4f34-8f20-60a2d34d97b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 1, 20, 0, 13, 17, 13, 20, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nntvKmZWLOpv",
        "outputId": "b907cde1-fd58-429e-9eca-c63f5eaa1968"
      },
      "source": [
        "type(trees)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYCW2MbMJXDr",
        "outputId": "51b6b242-96ae-486d-b943-7097cb92b203"
      },
      "source": [
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.48622945, 0.5137705 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 20), dtype=float32, numpy=\n",
              " array([[0.05069299, 0.04961899, 0.04831997, 0.04962244, 0.05026175,\n",
              "         0.0506347 , 0.05018949, 0.04982139, 0.05331923, 0.05073491,\n",
              "         0.05121638, 0.05068263, 0.04808311, 0.05126059, 0.04890196,\n",
              "         0.04874926, 0.0518473 , 0.04632362, 0.05115911, 0.04856017]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.5056311 , 0.49436888]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 23), dtype=float32, numpy=\n",
              " array([[0.04322062, 0.04179545, 0.04185909, 0.04485264, 0.04236766,\n",
              "         0.04236579, 0.04630024, 0.04411554, 0.04274567, 0.04441154,\n",
              "         0.0417267 , 0.0466552 , 0.04557517, 0.04108223, 0.0436444 ,\n",
              "         0.04425921, 0.04496698, 0.04318183, 0.0436477 , 0.04192689,\n",
              "         0.04115065, 0.04254494, 0.04560382]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.49855316, 0.5014469 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 23), dtype=float32, numpy=\n",
              " array([[0.04449129, 0.04237364, 0.04324834, 0.0440671 , 0.04283454,\n",
              "         0.04316343, 0.0444033 , 0.04454172, 0.04338585, 0.04317243,\n",
              "         0.04170939, 0.04616848, 0.04498308, 0.04142612, 0.04295998,\n",
              "         0.04290999, 0.04502702, 0.04397219, 0.04351544, 0.04157012,\n",
              "         0.0425739 , 0.04329684, 0.04420584]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 23), dtype=float32, numpy=\n",
              " array([[0.04406941, 0.04279422, 0.0456837 , 0.04464123, 0.0429058 ,\n",
              "         0.04350149, 0.04521213, 0.04604044, 0.04206172, 0.04364808,\n",
              "         0.04190242, 0.0445473 , 0.04679832, 0.04362074, 0.0441847 ,\n",
              "         0.04359429, 0.04074755, 0.04045793, 0.04381724, 0.04304034,\n",
              "         0.04288132, 0.04259113, 0.04125851]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 23), dtype=float32, numpy=\n",
              " array([[0.04271635, 0.04314357, 0.0459444 , 0.04449587, 0.0439612 ,\n",
              "         0.0436839 , 0.04447372, 0.04311709, 0.04282501, 0.04511143,\n",
              "         0.04414176, 0.04299787, 0.0452581 , 0.04131197, 0.0442808 ,\n",
              "         0.04038651, 0.04247866, 0.04546829, 0.04410326, 0.0443704 ,\n",
              "         0.03931678, 0.04486442, 0.04154859]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 23), dtype=float32, numpy=\n",
              " array([[0.04423836, 0.0420763 , 0.04651839, 0.04383924, 0.04409963,\n",
              "         0.04525253, 0.04446929, 0.04576313, 0.04136496, 0.04206874,\n",
              "         0.04279041, 0.04403322, 0.04709662, 0.04393474, 0.04516783,\n",
              "         0.04270371, 0.04011197, 0.04044355, 0.0423777 , 0.04377492,\n",
              "         0.04339082, 0.04304217, 0.0414419 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.50316435, 0.4968357 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.5094158 , 0.49058422]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.49936616, 0.50063384]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.51052684, 0.48947316]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[0.4929615 , 0.50703853]], dtype=float32)>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgPFobydJ50i",
        "outputId": "2b7065cc-1252-490f-9e04-74569cdf7073"
      },
      "source": [
        "seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 4, 1, 20, 0, 13, 17, 13, 20, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3PO3I8bKCUC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24517196-71df-48ef-917c-152b43e2f510"
      },
      "source": [
        "14,24\n",
        "14,1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfk9XcLNR0Ol",
        "outputId": "5015427c-7ebd-4f41-d345-38c057cb02b6"
      },
      "source": [
        "a = pad_seq(scores)\n",
        "a.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([15, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_qMaZmCnmE5",
        "outputId": "2788ea66-cda1-4f39-92ab-69529a55a8a9"
      },
      "source": [
        "b=one_hot_seq(seq,24)\n",
        "b.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([15, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNrZrvKxnyW8",
        "outputId": "4bdeed55-2ba2-4513-b1d0-73cb7ecf4f78"
      },
      "source": [
        "trees"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Module(Assign(Name, Call(Attribute(Call(Name)))))]"
            ]
          },
          "metadata": {},
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYXxcNCKqEIV",
        "outputId": "85212368-d173-4d45-e07b-7e043f82e5d0"
      },
      "source": [
        "model.compute_loss(trees[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.47753328084947"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "_pxyglh3waHg",
        "outputId": "f867576e-e9d2-4c89-b44f-0865f29a1894"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-220-4274cb293d2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    438\u001b[0m               'method accepts an `inputs` argument.')\n\u001b[1;32m    439\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m           raise ValueError('You cannot build your model by calling `build` '\n",
            "\u001b[0;32m<ipython-input-196-5479aa889c54>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-196-5479aa889c54>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# first encode the sequence of children recursively\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_children\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mh_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[0;32m--> 442\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute '_children'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8AH92Yq2Sofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PYTORCH"
      ],
      "metadata": {
        "id": "38rHhXPClawm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"Selecting GPU\")\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNfNUJEqpauV",
        "outputId": "73e1a122-674e-4c2a-be4b-d8dfa92edf35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2ONb5Tqysq4"
      },
      "source": [
        "import tree as tree\n",
        "import tree_grammar\n",
        "import python_ast_utils\n",
        "import numpy as np"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTM(torch.nn.Module):\n",
        "    \"\"\" A easier-to-use version of the gated recurrent unit (Cho et al., 2014).\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    _dim_in: int\n",
        "        The input dimensionality.\n",
        "    _dim_hid: int\n",
        "        The state dimensionality.\n",
        "    _gru: class torch.nn.GRU\n",
        "        The actual GRU network.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in, dim_hid):\n",
        "        super(SimpleLSTM, self).__init__()\n",
        "        self._dim_in = dim_in\n",
        "        self._dim_hid = dim_hid\n",
        "        self._dim_cell = dim_hid\n",
        "        self._lstm = torch.nn.LSTM(self._dim_in, self._dim_hid)\n",
        "\n",
        "    def forward(self, x, h = None):\n",
        "        \"\"\" Updates the state given the current state and the current input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: class torch.Tensor\n",
        "            The current input with dimensionality self._dim_in\n",
        "        h: class torch.Tensor (default = zero vector)\n",
        "            The current state with dimensionality self._dim_hid.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h: class torch.Tensor\n",
        "            The next state with dimensionality self._dim_hid, computed as\n",
        "            described above.\n",
        "\n",
        "        \"\"\"\n",
        "        # fill state with zeros if not given\n",
        "        if h is None:\n",
        "            h = torch.zeros(self._dim_hid)\n",
        "        # extend with empty dimensions\n",
        "        h = h.unsqueeze(0).unsqueeze(1)\n",
        "        # print(h.shape)\n",
        "        # extend x with empty dimensions\n",
        "        x = x.unsqueeze(0).unsqueeze(1)\n",
        "        # compute the next state\n",
        "        _, (h,c) = self._lstm(x, (h, h))\n",
        "        # return it\n",
        "        return h.squeeze(1).squeeze(0)"
      ],
      "metadata": {
        "id": "ZZvVoLex9zTs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RTGAE(torch.nn.Module):\n",
        "    \"\"\" An autoencoder for trees over a given grammar.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    grammar: class tree_grammar.TreeGrammar\n",
        "        The grammar from which trees are generated\n",
        "    dim: int\n",
        "        The encoding dimensionality.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, grammar, dim):\n",
        "        super(RTGAE, self).__init__()\n",
        "        self.grammar = grammar\n",
        "        self.dim = dim\n",
        "\n",
        "        # set up a tree parser for the given grammar. We need this for\n",
        "        # training the autoencoder\n",
        "        self.parser_ = tree_grammar.TreeParser(grammar)\n",
        "\n",
        "        # set up a map from symbols to indices\n",
        "        self.symbol_to_idx_ = {}\n",
        "        symbols = list(self.grammar._alphabet.keys())\n",
        "        for i in range(len(symbols)):\n",
        "            self.symbol_to_idx_[symbols[i]] = i\n",
        "\n",
        "        # set up a GRU to encode siblings\n",
        "        self.enc_siblings_ = torch.nn.LSTM(self.dim, self.dim)\n",
        "        # set up a GRU to encode to parents\n",
        "        self.enc_parents_  = SimpleLSTM(len(symbols), self.dim)\n",
        "        # set up a GRU to decode siblings\n",
        "        self.dec_siblings_ = SimpleLSTM(self.dim, self.dim)\n",
        "        self.dec_siblings_out_ = torch.nn.Linear(self.dim, self.dim)\n",
        "        # set up a GRU to decode from parents\n",
        "        self.dec_parents_  = SimpleLSTM(len(symbols), self.dim)\n",
        "        # set up linear layers to decide which grammar rule to take\n",
        "        # from a given nonterminal symbol\n",
        "        self.dec_cls_ = torch.nn.ModuleDict()\n",
        "        for nont in self.grammar._nonts:\n",
        "            self.dec_cls_[nont] = torch.nn.Linear(self.dim, len(self.grammar._rules[nont]))\n",
        "        # also set up classifiers for optional and starred rules\n",
        "        for lhs in self.grammar._rules:\n",
        "            for rhs in self.grammar._rules[lhs]:\n",
        "                for nont in rhs[1]:\n",
        "                    if nont not in self.dec_cls_ and isinstance(nont, str) and (nont.endswith('*') or nont.endswith('?')):\n",
        "                        self.dec_cls_[nont] = torch.nn.Linear(self.dim, 2)\n",
        "\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\" Encodes the given input tree recursively by first\n",
        "        encoding the child sequence to a vector and then the state\n",
        "        representing the child sequence to a parent state.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: class tree.Tree\n",
        "            An input tree.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        h: class torch.Tensor\n",
        "            A self.dim dimensional vectorial encoding of the input tree.\n",
        "\n",
        "        \"\"\"\n",
        "        # first encode the sequence of children recursively\n",
        "        if len(x._children) == 0:\n",
        "            h_child = torch.zeros(self.dim)\n",
        "        else:\n",
        "            H_child = torch.zeros(len(x._children), self.dim)\n",
        "            for k in range(len(x._children)):\n",
        "                H_child[k, :] = self.encode(x._children[k])\n",
        "            _, (h_child,_) = self.enc_siblings_(H_child.unsqueeze(1))\n",
        "            h_child = h_child.squeeze(1).squeeze(0)\n",
        "        # then, encode to a parent representation\n",
        "        label_encoding = torch.zeros(len(self.symbol_to_idx_))\n",
        "        label_encoding[self.symbol_to_idx_[x._label]] = 1.\n",
        "        h_parent = self.enc_parents_(label_encoding, h_child)\n",
        "        return h_parent\n",
        "\n",
        "\n",
        "    def decode(self, h, max_size = None):\n",
        "        \"\"\" Decodes the given vector into a tree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        h: class torch.Tensor\n",
        "            A self.dim dimensional vectorial encoding of some tree.\n",
        "        max_size: int (default = None)\n",
        "            A maximum number of decoding steps to prevent endless recursion.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x: class tree.Tree\n",
        "            The decoded tree.\n",
        "\n",
        "        \"\"\"\n",
        "        x, N = self.decode_(h, self.grammar._start, max_size)\n",
        "        return x\n",
        "\n",
        "    def decode_(self, h, nont, max_size = None):\n",
        "        \"\"\" Decodes the given vector into a tree.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        h: class torch.Tensor\n",
        "            A self.dim dimensional vectorial encoding of some tree.\n",
        "        nont: str\n",
        "            The nonterminal symbol from which decoding shall start.\n",
        "        max_size: int (default = None)\n",
        "            A maximum number of decoding steps to prevent endless recursion.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x: class tree.Tree\n",
        "            The decoded tree.\n",
        "        N: int\n",
        "            The size of the decoded tree.\n",
        "\n",
        "        \"\"\"\n",
        "        # first, decide the grammar rule that shall be applied\n",
        "        r = torch.argmax(self.dec_cls_[nont](h))\n",
        "        rule = self.grammar._rules[nont][r]\n",
        "        # generate a new tree object with the label determined by the\n",
        "        # grammar rule\n",
        "        label = rule[0]\n",
        "        child_nonts = rule[1]\n",
        "        x = tree.Tree(label)\n",
        "        # map from parent to child state\n",
        "        label_encoding = torch.zeros(len(self.symbol_to_idx_))\n",
        "        label_encoding[self.symbol_to_idx_[x._label]] = 1.\n",
        "        h_child = self.dec_parents_(label_encoding, h)\n",
        "        # generate all children that are determined by the grammar rule\n",
        "        child_nonts = rule[1]\n",
        "        N = 1\n",
        "        if max_size is not None:\n",
        "            max_size -= 1\n",
        "        for k in range(len(child_nonts)):\n",
        "            if isinstance(child_nonts[k], str) and child_nonts[k].endswith('*'):\n",
        "                # if the next child is starred, continue decoding children for\n",
        "                # this nonterminal until the classifier says otherwise\n",
        "                while max_size is None or max_size > 0:\n",
        "                    continue_flag = torch.argmax(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                    if continue_flag == 0:\n",
        "                        break\n",
        "                    # infer state for the next child\n",
        "                    hk = self.dec_siblings_out_(h_child)\n",
        "                    # decode it recursively and append it\n",
        "                    y, Nk = self.decode_(hk, child_nonts[k][:-1], max_size)\n",
        "                    x._children.append(y)\n",
        "                    # adjust the remaining maximum size\n",
        "                    if max_size is not None:\n",
        "                        max_size -= Nk\n",
        "                    N += Nk\n",
        "                    # update the state\n",
        "                    h_child = self.dec_siblings_(hk, h_child)\n",
        "                # once all children for the starred nonterminal are decoded,\n",
        "                # continue with the next child nonterminal\n",
        "                continue\n",
        "            elif isinstance(child_nonts[k], str) and child_nonts[k].endswith('?'):\n",
        "                # if the next child is optional, check if it shall be decoded\n",
        "                continue_flag = torch.argmax(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                if continue_flag == 0:\n",
        "                    # if not, continue right away\n",
        "                    continue\n",
        "                nont = child_nonts[k][:-1]\n",
        "            else:\n",
        "                nont = child_nonts[k]\n",
        "            # if the max size is exceeded, leave the nonterminals\n",
        "            if max_size is not None and max_size <= 0:\n",
        "                x._children.append(tree.Tree(nont))\n",
        "                continue\n",
        "            # at this point, we know that the child shall be decoded regularly.\n",
        "            # First, we infer the state for the next child\n",
        "            hk = self.dec_siblings_out_(h_child)\n",
        "            # then, we decode it recursively and append it\n",
        "            y, Nk = self.decode_(hk, nont, max_size)\n",
        "            x._children.append(y)\n",
        "            # adjust the remaining maximum size\n",
        "            if max_size is not None:\n",
        "                max_size -= Nk\n",
        "            N += Nk\n",
        "            # update the state\n",
        "            h_child = self.dec_siblings_(hk, h_child)\n",
        "        # return\n",
        "        return x, N\n",
        "\n",
        "    def decode_forced_(self, h, nont, seq, t, score_list, decoding_list = None):\n",
        "        \"\"\" Decodes the given vector into a tree, forced by the given sequence\n",
        "        of rules and accumulates the rule scores of the model along the way.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        h: class torch.Tensor\n",
        "            A self.dim dimensional vectorial encoding of some tree.\n",
        "        nont: str\n",
        "            The nonterminal symbol from which decoding shall start.\n",
        "        seq: list\n",
        "            A sequence of grammar rules.\n",
        "        t: int\n",
        "            The current index in seq.\n",
        "        score_list: list\n",
        "            The output list of scores.\n",
        "        decoding_list: list (default = None)\n",
        "            The output list for decodings along the way.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        t: int\n",
        "            The index in seq after decoding h.\n",
        "\n",
        "        \"\"\"\n",
        "        # first, decide the current scores\n",
        "        if decoding_list is not None:\n",
        "            decoding_list.append(h)\n",
        "        score_list.append(self.dec_cls_[nont](h))\n",
        "        # then, retrieve the rule that ought to be applied\n",
        "        r = seq[t]\n",
        "        t += 1\n",
        "        rule = self.grammar._rules[nont][r]\n",
        "        # generate a new tree object with the label determined by the\n",
        "        # grammar rule\n",
        "        label = rule[0]\n",
        "        child_nonts = rule[1]\n",
        "        # map from parent to child state\n",
        "        label_encoding = torch.zeros(len(self.symbol_to_idx_))\n",
        "        label_encoding[self.symbol_to_idx_[label]] = 1.\n",
        "        h_child = self.dec_parents_(label_encoding, h)\n",
        "        # generate all children that are determined by the grammar rule\n",
        "        child_nonts = rule[1]\n",
        "        for k in range(len(child_nonts)):\n",
        "            if isinstance(child_nonts[k], str) and child_nonts[k].endswith('*'):\n",
        "                # if the next child is starred, continue decoding children for\n",
        "                # this nonterminal until the rule sequence says otherwise\n",
        "                score_list.append(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                while seq[t] == 1:\n",
        "                    t += 1\n",
        "                    # infer state for the next child\n",
        "                    hk = self.dec_siblings_out_(h_child)\n",
        "                    # decode the current child\n",
        "                    t = self.decode_forced_(hk, child_nonts[k][:-1], seq, t, score_list, decoding_list)\n",
        "                    # update the state\n",
        "                    h_child = self.dec_siblings_(hk, h_child)\n",
        "                    score_list.append(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                t += 1\n",
        "                # once all children for the starred nonterminal are decoded,\n",
        "                # continue with the next child nonterminal\n",
        "                continue\n",
        "            elif isinstance(child_nonts[k], str) and child_nonts[k].endswith('?'):\n",
        "                # if the next child is optional, check if it shall be decoded\n",
        "                score_list.append(self.dec_cls_[child_nonts[k]](h_child))\n",
        "                if seq[t] == 0:\n",
        "                    t += 1\n",
        "                    continue\n",
        "                t += 1\n",
        "                nont = child_nonts[k][:-1]\n",
        "            else:\n",
        "                nont = child_nonts[k]\n",
        "            # at this point, we know that the child shall be decoded regularly.\n",
        "            # First, we infer the state for the next child\n",
        "            hk = self.dec_siblings_out_(h_child)\n",
        "            # then, we decode it recursively\n",
        "            t = self.decode_forced_(hk, nont, seq, t, score_list, decoding_list)\n",
        "            # update the state\n",
        "            h_child = self.dec_siblings_(hk, h_child)\n",
        "        # return\n",
        "        return t\n",
        "\n",
        "    def compute_loss(self, x):\n",
        "        \"\"\" Autoencodes the given tree and computes the classification loss\n",
        "        between selecting the correct node labels and the choice of the current\n",
        "        model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: class tree.Tree\n",
        "            An input tree.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss: class torch.Tensor\n",
        "            The classification loss for choosing the correct node labels.\n",
        "\n",
        "        \"\"\"\n",
        "        # encode the tree\n",
        "        h = self.encode(x)\n",
        "        # find the actual rule sequence that should be chosen by using a\n",
        "        # tree parser\n",
        "        nont, seq = self.parser_.parse_tree(x)\n",
        "        # find the rules that the current autoencoder would have chosen instead\n",
        "        scores = []\n",
        "        self.decode_forced_(h, nont, seq, 0, scores)\n",
        "        # transform the actual score list and the desired chosen rules\n",
        "        # into torch tensors\n",
        "        scores = torch.nn.utils.rnn.pad_sequence(scores, True)\n",
        "        seq    = torch.tensor(seq, dtype=torch.long)\n",
        "        # compute crossentropy loss\n",
        "        loss = torch.nn.functional.cross_entropy(scores, seq, reduction = 'sum')\n",
        "        # return it\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encode(x)\n",
        "        decoded = self.decode(encoded)\n",
        "        return decoded\n",
        "    \n",
        "\n",
        "    # def compute_acc(self, x):\n",
        "        "
      ],
      "metadata": {
        "id": "rUiJB0oW90_q"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# load the data first. We create a list of all programs and a list of traces,\n",
        "# where each trace is a list of program indices\n",
        "programs     = ['']\n",
        "traces       = []\n",
        "# start loading the data\n",
        "student_dirs = list(sorted(os.listdir('mock_dataset')))\n",
        "for student_dir in student_dirs:\n",
        "    # initialize a new trace for the student which starts at the empty program\n",
        "    trace = [0]\n",
        "    # load all steps of this student\n",
        "    steps = list(sorted(os.listdir(f'mock_dataset/{student_dir}')))\n",
        "    for step in steps:\n",
        "        # load the current program\n",
        "        with open(f'mock_dataset/{student_dir}/{step}') as program_file:\n",
        "            trace.append(len(programs))\n",
        "            programs.append(program_file.read())\n",
        "    # append the trace\n",
        "    traces.append(trace)\n",
        "print('read %d traces with %d programs' % (len(traces), len(programs)))"
      ],
      "metadata": {
        "id": "3qvC94gx92fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "XX = np.random.rand(1000000,256)\n",
        "XX = ( (XX  - 0 ) / (1 - 0) ) * (1 - -1) + -1"
      ],
      "metadata": {
        "id": "Gl37rPijf5UI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast2vec\n",
        "model = ast2vec.load_model('ast2vec.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "M53HgfWCgCFv",
        "outputId": "bc308caf-295f-4b98-c093-3415c15f6a4a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5b16f1e5e71a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mast2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ast2vec.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/drive/MyDrive/ast2vec_tf/ast2vec.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmodel\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mrtgae2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRTGAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_ast_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDIM_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# load the model parameters into it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# return the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1498\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1499\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for RTGAE:\n\tsize mismatch for enc_parents_._gru.weight_ih_l0: copying a param with shape torch.Size([768, 90]) from checkpoint, the shape in current model is torch.Size([768, 91]).\n\tsize mismatch for dec_parents_._gru.weight_ih_l0: copying a param with shape torch.Size([768, 90]) from checkpoint, the shape in current model is torch.Size([768, 91]).\n\tsize mismatch for dec_cls_.expr.weight: copying a param with shape torch.Size([23, 256]) from checkpoint, the shape in current model is torch.Size([24, 256]).\n\tsize mismatch for dec_cls_.expr.bias: copying a param with shape torch.Size([23]) from checkpoint, the shape in current model is torch.Size([24])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[]\n",
        "with open(\"dataset.txt\") as file:\n",
        "\n",
        "    for line in file:\n",
        "        data.append(line.strip())\n",
        "        "
      ],
      "metadata": {
        "id": "4zY1PGbtWzvu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRAipsejZXtO",
        "outputId": "90e43be8-d715-45cf-9ffa-b8f91d6b60e3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "496009"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import python_ast_utils\n",
        "\n",
        "trees = python_ast_utils.parse_asts(data[:1000],verbose=True)\n",
        "print('After compilation, %d unique syntax trees were left' % len(trees))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiaEMo2aSaFw",
        "outputId": "3137062e-e070-47f0-ce99-791183e261ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting parsing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1000/1000 [00:02<00:00, 354.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After compilation, 1000 unique syntax trees were left\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trees)\n",
        "#type(trees[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_pC94p6fbsZ",
        "outputId": "a322e7a7-2cff-4a18-e304-5a5fd814686e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = RTGAE(python_ast_utils.grammar, dim = 256)\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=1e-3, \n",
        "                             weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "B1EJqrsY-JUi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "P6cj8Rdxfaie"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec= model.encode(trees[2])\n",
        "print(vec)"
      ],
      "metadata": {
        "id": "MIlFjm9399F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "from tqdm import tqdm\n",
        "# programs     = ['arr = input().split()\\n', 'arr = input().split()\\n', 'arr = input().split()\\n']\n",
        "# trees, programs_to_trees = python_ast_utils.parse_asts(programs, filter_uniques = True)\n",
        "outputs = []\n",
        "progbar=tqdm\n",
        "for epoch in range(num_epochs):\n",
        "    for x in progbar(range(len(trees[:100]))):\n",
        "        loss = model.compute_loss(trees[x])\n",
        "        #acc = model.compute_acc(x)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print(loss)\n",
        "        # i = 0\n",
        "    \n",
        "    # for x in val_trees:\n",
        "    #   val_loss = model.compute_loss(x)\n",
        "    #   val_acc = model.compute_acc(x)    \n",
        "\n",
        "\n",
        "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')#, VAL Loss:{val_loss.item():.4f}')\n",
        "    outputs.append((epoch, loss))\n"
      ],
      "metadata": {
        "id": "IP_ISHWKHTDY",
        "outputId": "11edb414-1fa2-4ec4-aa9b-2c2950306df9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:20<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1, Loss:62.2686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:20<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2, Loss:57.1091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:26<00:00,  1.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:3, Loss:54.7787\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:17<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:4, Loss:56.6904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:18<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:5, Loss:55.1093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:14<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:6, Loss:53.5669\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:19<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:7, Loss:51.4686\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:20<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:8, Loss:48.4968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:21<00:00,  1.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:9, Loss:48.2614\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 100/100 [01:21<00:00,  1.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:10, Loss:45.7326\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7NhOxInknW3I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}